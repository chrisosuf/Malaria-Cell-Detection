{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.morphology as skm\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf') # order channels by tensorflow\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "\n",
    "# Acknowledgements: Keras and Tensorflow Documentation, Anuj Shah and Siraj Ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading dataset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Define data path\n",
    "data_path = '/Convolution/sample'\n",
    "data_dir_list = os.listdir(data_path) #Folders\n",
    "\n",
    "#grayscale, tophat+grayscale, 2 tophat+grayscale, tophat color, and just color (original image)\n",
    "\n",
    "# Define image size to feed into network\n",
    "img_rows=128\n",
    "img_cols=128\n",
    "\n",
    "img_data_list=[]\n",
    "\n",
    "# Go through folders of pictures\n",
    "for dataset in data_dir_list:\n",
    "    # filter out hidden files\n",
    "    if not dataset.startswith('.'):    \n",
    "        img_list = os.listdir(data_path + '/' + dataset)\n",
    "        print('Loading dataset')\n",
    "        # in each image in each folder: read it, manipulate it, and append it to list\n",
    "        for img in img_list:\n",
    "            try:\n",
    "                # filter hidden files\n",
    "                if not img.startswith('.'):\n",
    "                    input_img = cv2.imread(data_path + '/' + dataset + '/' + img) # read\n",
    "                    \n",
    "                    #input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "                    \n",
    "                    #black tophat approach (1 and 2 rounds)\n",
    "                    black_tophat = skm.black_tophat(input_img)\n",
    "                    black_tophat2 = skm.black_tophat(black_tophat)\n",
    "                    input_img = cv2.cvtColor(black_tophat2, cv2.COLOR_BGR2GRAY) # convert tophat to grayscale\n",
    "\n",
    "                    #convert to binary (not the best, made entire img blob occasionaly)\n",
    "                    #(thresh, im_bw) = cv2.threshold(input_img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "                    input_img_resize = cv2.resize(input_img,(128, 128)) # resize (use im_bw if binary, input_img otherwise)\n",
    "                    img_data_list.append(input_img_resize) # append to list\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x13F839C50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining images\n",
    "\n",
    "from PIL import Image\n",
    "im_gray = img_data_list[300]\n",
    "\n",
    "#testing binary\n",
    "#(thresh, im_bw) = cv2.threshold(im_gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "Image.fromarray(im_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "img_data = np.array(img_data_list) # convert into an array\n",
    "img_data = img_data.astype('float32') # convert into a float\n",
    "img_data /= 255 # normalize\n",
    "print (img_data.shape)\n",
    "\n",
    "#reshape backend for tensorflow, \n",
    "#start at axis=4: ordering of samples, channels, rows, and columns\n",
    "img_data= np.expand_dims(img_data, axis=4) # didnt expand dimensions for color to keep the 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Assigning Labels\n",
    "\n",
    "# Define the number of classes (Parasitized, Uninfected)\n",
    "num_classes = 2\n",
    "\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:1000]=1 #Parasitized\n",
    "labels[1000:]=0 #Uninfected\n",
    "\n",
    "names = ['Parasitized','Uninfected']\n",
    "# convert class labels to on-hot encoding\n",
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "\n",
    "# Shuffle the dataset, random state gives a fixed value of shuffling\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(128, 128,..., padding=\"same\")`\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 127008)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8128576   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,138,274\n",
      "Trainable params: 8,138,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the convolutional neural network\n",
    "\n",
    "input_shape=img_data[0].shape #for grayscale\n",
    "#input_shape=(128, 128, 3) #for color \n",
    "model = Sequential()\n",
    "\n",
    "# for first convolutional layer we use 32 filters, 3x3 rows by columns\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=input_shape))\n",
    "model.add(Activation('relu')) #relu\n",
    "\n",
    "# second layer layer\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling of size 2x2\n",
    "model.add(Dropout(0.5)) # Dropout to avoid overfitting\n",
    "\n",
    "# Flatten into an open one dimensional vector which becomes our feature vector to feed into the fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# number of classes is 2\n",
    "model.add(Dense(num_classes))\n",
    "# softmax regression\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile by giving a loss function:\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 88s 55ms/step - loss: 0.6292 - acc: 0.6244 - val_loss: 0.5960 - val_acc: 0.6575\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 82s 52ms/step - loss: 0.6025 - acc: 0.6550 - val_loss: 0.5923 - val_acc: 0.6275\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 82s 52ms/step - loss: 0.6034 - acc: 0.6556 - val_loss: 0.5490 - val_acc: 0.8200\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 83s 52ms/step - loss: 0.5543 - acc: 0.7056 - val_loss: 0.5284 - val_acc: 0.6325\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.5204 - acc: 0.7469 - val_loss: 0.5274 - val_acc: 0.9075\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 80s 50ms/step - loss: 0.4668 - acc: 0.8000 - val_loss: 0.4963 - val_acc: 0.6775\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 83s 52ms/step - loss: 0.4548 - acc: 0.8187 - val_loss: 0.4205 - val_acc: 0.6500\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 81s 51ms/step - loss: 0.4440 - acc: 0.8462 - val_loss: 0.3863 - val_acc: 0.8800\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 80s 50ms/step - loss: 0.4470 - acc: 0.8738 - val_loss: 0.3857 - val_acc: 0.8825\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 80s 50ms/step - loss: 0.4375 - acc: 0.8475 - val_loss: 0.6144 - val_acc: 0.6725\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 83s 52ms/step - loss: 0.3687 - acc: 0.8988 - val_loss: 0.3682 - val_acc: 0.9150\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.4135 - acc: 0.8981 - val_loss: 0.3719 - val_acc: 0.9100\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 86s 53ms/step - loss: 0.4193 - acc: 0.9125 - val_loss: 0.3748 - val_acc: 0.9175\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 82s 52ms/step - loss: 0.3689 - acc: 0.8950 - val_loss: 0.3389 - val_acc: 0.9075\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 81s 50ms/step - loss: 0.3053 - acc: 0.9169 - val_loss: 0.3041 - val_acc: 0.9025\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.3084 - acc: 0.9131 - val_loss: 0.2963 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.2746 - acc: 0.9275 - val_loss: 0.3602 - val_acc: 0.9125\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.2806 - acc: 0.9194 - val_loss: 0.3067 - val_acc: 0.9400\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.2494 - acc: 0.9350 - val_loss: 0.2804 - val_acc: 0.9400\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 82s 51ms/step - loss: 0.3140 - acc: 0.9325 - val_loss: 0.3216 - val_acc: 0.9425\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "#grayscale is 1 channel 3 channels is rgb\n",
    "num_channel=1\n",
    "num_epoch=20 # only 20 epochs so did 16 batches\n",
    "batch_size = 16 \n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epoch, verbose=1, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "#model.save('Convolution/model/my_model.h5')\n",
    "\n",
    "# returns saved model using 2 rounds of black top hat (94.25% accuracy)\n",
    "model = load_model('/Users/Administrator/Desktop/Convolution/model/two_tophat_grayscale.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3216337168216705\n",
      "Test accuracy: 0.9425\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      " class 0(uninfected)       0.92      0.97      0.94       201\n",
      "class 1(parasitized)       0.96      0.92      0.94       199\n",
      "\n",
      "           micro avg       0.94      0.94      0.94       400\n",
      "           macro avg       0.94      0.94      0.94       400\n",
      "        weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "[[194   7]\n",
      " [ 16 183]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the confusion matrix and evaluating accuracy\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "#test loss and accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "target_names = ['class 0(uninfected)', 'class 1(parasitized)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
